{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOG 전처리 기법 추가 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "976617e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "# sys.path.append(\"C:/Users/hk-Lee/AppData/Local/Programs/Python/Python310/Lib/site-packages\")\n",
    "# sys.path\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b11f7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('C:/Users/유재림/Desktop/ML23_2-main/archive/fashion-mnist_train.csv')\n",
    "\n",
    "train_y = training_data['label']\n",
    "\n",
    "train_X = training_data.drop('label',axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2639c403",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:19<00:00, 3019.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.10,\n",
    "    height_shift_range=0.10\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Data augmentation and adding to the training set loop\n",
    "aug_train_X = []\n",
    "aug_train_y = []\n",
    "\n",
    "for index, row in tqdm(train_X.iterrows(), total=len(train_X)):\n",
    "    random_num = np.random.random()\n",
    "\n",
    "    img = row.values.reshape((28, 28, 1))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Original data\n",
    "    aug_train_X.append(img_array[0])\n",
    "    aug_train_y.append(train_y[index])\n",
    "\n",
    "    # Augmented data with 2/3 probability\n",
    "    \n",
    "    augmented_img_array = next(datagen.flow(img_array, batch_size=1))\n",
    "    augmented_img_array = augmented_img_array.squeeze(axis=0)\n",
    "    aug_train_X.append(augmented_img_array)\n",
    "    aug_train_y.append(train_y[index])\n",
    "\n",
    "print(len(aug_train_X))\n",
    "print(len(aug_train_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b5e8e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [00:17<00:00, 3367.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of aug_train_X: (180000, 28, 28, 1)\n",
      "Shape of aug_train_y: (180000,)\n"
     ]
    }
   ],
   "source": [
    "for index, row in tqdm(train_X.iterrows(), total=len(train_X)):\n",
    "    random_num = np.random.random()\n",
    "\n",
    "    img = row.values.reshape((28, 28, 1))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "    # Original data\n",
    "    # aug_train_X.append(img_array[0])\n",
    "    # aug_train_y.append(train_y[index])\n",
    "\n",
    "    augmented_img_array = next(datagen.flow(img_array, batch_size=1))\n",
    "    augmented_img_array = augmented_img_array.squeeze(axis=0)\n",
    "    aug_train_X.append(augmented_img_array)\n",
    "    aug_train_y.append(train_y[index])\n",
    "    \n",
    "# Convert the lists to numpy arrays\n",
    "aug_train_X = np.array(aug_train_X)\n",
    "aug_train_y = np.array(aug_train_y)\n",
    "\n",
    "# Check the shape of the new arrays\n",
    "print(\"Shape of aug_train_X:\", aug_train_X.shape)\n",
    "print(\"Shape of aug_train_y:\", aug_train_y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 호그 특징 처리 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bf50c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "\n",
    "# Function to apply HOG transformation and combine with original features\n",
    "def apply_combined_feature_extraction(images, pca_components=256):\n",
    "    combined_features = []\n",
    "    for img in images:\n",
    "        # Reshape the image to its original shape after PCA\n",
    "        img_reshaped = img.reshape((int(np.sqrt(pca_components)), int(np.sqrt(pca_components))))\n",
    "\n",
    "        # Apply HOG transformation\n",
    "        _, hog_feature = hog(img_reshaped, orientations=8, pixels_per_cell=(8, 8), cells_per_block=(2, 2), visualize=True)\n",
    "\n",
    "        # 호그 특징 펼친거 \n",
    "        hog_feature_flattened = hog_feature.flatten()\n",
    "\n",
    "        # 오리지널 특징 \n",
    "        original_feature_flattened = img.flatten()\n",
    "\n",
    "        # 원래 특징 + 호그 특징 \n",
    "        combined_feature = np.concatenate((original_feature_flattened, hog_feature_flattened))\n",
    "\n",
    "        combined_features.append(combined_feature) # 혼합 특징 \n",
    "\n",
    "    return np.array(combined_features)\n",
    "\n",
    "# Assuming aug_train_X is your original dataset\n",
    "aug_train_X = aug_train_X.reshape((aug_train_X.shape[0], -1))\n",
    "aug_train_X /= 255.0\n",
    "\n",
    "# Apply combined feature extraction\n",
    "aug_train_X_combined = apply_combined_feature_extraction(aug_train_X)\n",
    "\n",
    "# Now, aug_train_X_combined contains both original and HOG features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c3b56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(180000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(aug_train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58971064",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=256)\n",
    "aug_train_X_pca = pca.fit_transform(aug_train_X)\n",
    "\n",
    "# Apply HOG transformation to PCA features\n",
    "aug_train_X_hog_pca = apply_hog_transform(aug_train_X_pca)\n",
    "\n",
    "# Combine PCA and HOG features\n",
    "aug_train_X_combined = np.hstack((aug_train_X_pca, aug_train_X_hog_pca))\n",
    "\n",
    "# Normalize the combined features\n",
    "scaler = StandardScaler()\n",
    "aug_train_X_combined = scaler.fit_transform(aug_train_X_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3ea363",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(gamma='scale', kernel='rbf', C=13)\n",
    "svc.fit(aug_train_X_combined, aug_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9f87be",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_folder = \"C:/Users/유재림/Desktop/ML23_2-main/ML23_2/submitcode/data\"\n",
    "\n",
    "# 테스트 데이터 로드 및 전처리\n",
    "test_X = []\n",
    "file_names = []\n",
    "\n",
    "print(len(os.listdir(test_data_folder)))\n",
    "# tqdm으로 래핑\n",
    "for file_name in tqdm(os.listdir(test_data_folder), desc=\"Loading and preprocessing\"):\n",
    "    if file_name.endswith(\".png\"):\n",
    "    \n",
    "        file_path = str(test_data_folder) + \"/\" + str(file_name)\n",
    "        try:\n",
    "\n",
    "            img_array = np.fromfile(file_path, np.uint8)\n",
    "            img = cv2.imdecode(img_array, cv2.IMREAD_GRAYSCALE)\n",
    "            image_array = img / 255.0  # 0부터 1사이의 값\n",
    "            test_X.append(image_array.flatten())  # 2D 배열을 1D로 펼침\n",
    "            \n",
    "            file_names.append(file_name)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "# NumPy 배열로 변환\n",
    "test_X = np.array(test_X)\n",
    "print(test_X.shape)\n",
    "\n",
    "if len(test_X) == 0:\n",
    "    print(\"No valid test images found.\")\n",
    "else:\n",
    "    print(\"pca 적용\")\n",
    "    test_X_pca = pca.transform(test_X)\n",
    "    print(test_X_pca.shape)\n",
    "    predictions = svc.predict(test_X_pca)\n",
    "\n",
    "with open('testResult3.txt', 'w') as file:\n",
    "    for i, pred in enumerate(predictions):\n",
    "        file.write(f\"{i:05d} {pred}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfdaf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc\n",
    "from collections import Counter\n",
    "\n",
    "testResult_path = \"C:/Users/유재림/Desktop/ML23_2-main/ML23_2/submitcode/testResult3.txt\"\n",
    "label_path = \"C:/Users/유재림/Desktop/ML23_2-main/ML23_2/submitcode/label.txt\"\n",
    "\n",
    "# pred에 해당하는 testResult.txt 파일 읽어오는 부분입니다.\n",
    "with open(testResult_path, 'r') as file1:\n",
    "    preds = file1.readlines()\n",
    "\n",
    "# 정답에 해당하는 label.txt 파일 읽어오는 부분입니다.\n",
    "with open(label_path, 'r') as file2:\n",
    "    labels = file2.readlines()\n",
    "    \n",
    "\n",
    "# pred와 label의 클래스값만 리스트로 변환하는 부분입니다.\n",
    "p = np.array([pred.strip().split()[1] for pred in preds])\n",
    "l = np.array([label.strip().split()[1] for label in labels])\n",
    "\n",
    "# pred의 클래스 개수를 count하는 부분입니다.\n",
    "predict_label_count_dict = Counter(p)\n",
    "predict_label_count_dict = dict(sorted(predict_label_count_dict.items()))\n",
    "\n",
    "## mAP 계산하는 부분입니다.\n",
    "AP = []\n",
    "num_class = 10\n",
    "\n",
    "# 모든 클래스에 대해 반복\n",
    "for c, freq in predict_label_count_dict.items() :\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "\n",
    "    temp_precision = []\n",
    "    temp_recall = []\n",
    "    \n",
    "    for i in range(len(p)):\n",
    "        # TP, FN 계산\n",
    "        if l[i] == c and p[i] == c :\n",
    "            TP += 1\n",
    "        elif l[i] != c and p[i] == c :\n",
    "            FN += 1\n",
    "        \n",
    "        # preciison, recall 계산            \n",
    "        if TP+FN != 0: \n",
    "            temp_precision.append(TP/(TP+FN))\n",
    "            temp_recall.append(TP/freq)\n",
    "\n",
    "    # AP 배열에 클래스 각각의 AP value 저장\n",
    "    # auc : preciison-recall curve의 면적 구해줌\n",
    "    AP.append(auc(temp_recall, temp_precision))\n",
    "    \n",
    "mAP = sum(AP) / num_class\n",
    "\n",
    "# 각각의 클래스에 대한 AP와 mAP의 Table 출력 부분입니다.\n",
    "class_name = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "table = \"| {:<13} | {:<13} |\\n\".format(\"Class\", \"AP\") + \"|---------------|---------------|\\n\"\n",
    "\n",
    "for c_name, ap in zip(class_name, AP):\n",
    "    table += \"| {:<13} | {:<13.4f} |\\n\".format(c_name, ap)\n",
    "\n",
    "table += \"| {:<13} | {:<13.4f} |\\n\".format(\"mAP\", mAP)\n",
    "\n",
    "print(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
